---
title: "STA286 Lecture 26"
author: "Neil Montgomery"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  beamer_presentation:
    keep_tex: TRUE
    incremental: TRUE
#    df_print: tibble
    fig_caption: FALSE
classoption: aspectratio=169
header-includes:
- \renewcommand{\le}{\leqslant}
- \renewcommand{\ge}{\geqslant}
- \renewcommand\P[1]{P{\left(#1\right)}}
- \newcommand\F[1]{F_{\tiny{#1}}}
- \newcommand\f[1]{f_{\tiny{#1}}}
- \newcommand\p[1]{p_{\tiny{#1}}}
- \newcommand\M[1]{M_{\tiny{#1}}}
- \newcommand\V[1]{\text{Var}\!\left(#1\right)}
- \newcommand\E[1]{E\!\left(#1\right)}
- \newcommand\N[1]{N_{\tiny{#1}}}
- \newcommand\ol{\overline}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE,
                      dev = 'pdf')
options(tibble.width=70, tibble.print_max=5)
library(tidyverse)
```

## the two-sample problem (normal populations) with equal variances

We have two populations $N(\mu_1,\sigma)$ and $N(\mu_2,\sigma)$, and the goal is to estimate $\theta = \mu_1 - \mu_2$.

Gather independent samples: $X_{11},\ldots,X_{1n_1}$ i.i.d. $N(\mu_1,\sigma)$ and $X_{21},\ldots,X_{2n_2}$ i.i.d. $N(\mu_2,\sigma)$.

\pause The "obvious" estimator is $\ol{X}_1 - \ol{X}_2$, which will have a normal distribution with:
\begin{align*}
\E{\ol{X}_1 - \ol{X}_2} \onslide<3->{&= \mu_1 - \mu_2}\\
\onslide<4->{\V{\ol{X}_1 - \ol{X}_2}} \onslide<5->{&= \V{\ol{X}_1} + \V{\ol{X}_2} = \frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2}} \onslide<6->{= \sigma^2\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}
\end{align*}

\pause\pause\pause\pause We need to figure out what to do about $\sigma^2$.

## for $\sigma^2$, use the data from both samples

The sample variances $S^2_1$ and $S^2_2$ are both unbiased estimators for $\sigma^2$, so any weighted average (with weights that add up to 1) of them will also be an unbiased estimator.

\pause We call this the pooled variance estimator:
$$S^2_p = \frac{(n_1 - 1)S_1^2}{(n_1 - 1) + (n_2 - 1)} + \frac{(n_2 - 1)S_2^2}{(n_1 - 1) + (n_2 - 1)}$$

$$S^2_p = \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}$$

\pause Note that when $n_1=n_2$ this is just the average of the two sample variances.

## putting it all together

We want an interval estimator for $\mu_1 - \mu_2$. So far we have:
$$\frac{\left(\ol{X}_1 - \ol{X}_2\right) - (\mu_1 - \mu_2)}{\sigma\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim N(0,1)$$

\pause Who wants to guess what the distribution of this will be:
$$\frac{\left(\ol{X}_1 - \ol{X}_2\right) - (\mu_1 - \mu_2)}{S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \sim \onslide<3->{t}\onslide<4->{_{n_1 + n_2 - 2}}$$

\pause \pause Isolating $\mu_1-\mu_2$ in the usual way gives the confidence interval formula.

## two normal samples, equal variances C.I.

$$\left(\ol{X}_1 - \ol{X}_2\right) \pm t_{n_1+n_2-2, \alpha/2}\,S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$$

In the 95\% case, another instance of my patented:
$$\text{estimator} \pm \text{``}2\text{''} \text{s.e.(estimator)}$$

## example - watching two kinds of paint dry

If the world of one brand of paint drying was too fast-paced, this example is for you. (Question 9.49 from the textbook.)  Two brands of paint will have their drying times compared. 

\pause The goal is to estimate the difference between the mean drying times.

\pause Here's a glance at the "dataset" as is, organized in a way it should never be collected:

```{r}
library(tidyverse)
paint <- as_tibble(read.delim("Ex09.49.txt"))
paint
```

## watching two kinds of paint dry

A \text{real} dataset looks like this:

```{r}
paint <- gather(paint, key = brand, value = time)
paint
```

## watching two kinds of paint dry

Anyway, here is a summary of the two groups:

```{r}
library(knitr)
options(digits=3)
paint %>% 
  group_by(brand) %>% 
  summarize(x_bar = mean(time), samp_var = var(time), n=n()) %>% kable
```

```{r}
paint_fit <- paint %>% t.test(time ~ brand, data=., var.equal=TRUE) 
df <- paint_fit$parameter
lower <- paint_fit$conf.int[1]
upper <- paint_fit$conf.int[2]
```

The degrees of freedom is `r df`. The number from the $t$ distribution is $t_{`r df`, 0.025} = `r -qt(0.025, df)`$.

The 95\% confidence interval is $[`r lower`, `r upper`]$.

## when the variances cannot be assumed to be equal

Most two-sample analyses in practice don't bother with the equal variance assumption, and just use the following sequence of facts.

$$\frac{\left(\ol{X}_1 - \ol{X}_2\right) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim N(0,1)$$

\pause

$$\frac{\left(\ol{X}_1 - \ol{X}_2\right) - (\mu_1 - \mu_2)}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}} \sim^{approx} t_\nu$$

where $\nu$ has one of the most disgraceful formulae in the history of formulae. Don't look at its formula on the next slide.

## what did I just tell you?

$$\nu = \frac{\left(\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}\right)^2}{\frac{\left(S_1^2/n_1\right)^2}{n_1-1} + \frac{\left(S_2^2/n_2\right)^2}{n_2-1}}$$

This formula is not for humans to use.

But there are few things to notice about it:

* if $S_1^2 \approx S_2^2$, then $\nu \approx n_1 + n_2 - 2$.

* if $S_1^2 \ll S_2^2$, then $\nu \approx n_2 - 1$, and vice versa.

\pause It won't usually be an integer, so if you need to use this method on a test (where I'd give you the value of $\nu$), just use whatever nearby integer that is convenient.

## watching two kinds of paint dry, now in a very slightly different way

The C.I. formula becomes:

$$\left(\ol{X}_1 - \ol{X}_2\right) \pm t_{/nu, \alpha/2}\,\sqrt{\frac{S^2_1}{n_1} + \frac{S^2_2}{n_2}}$$

which in the 95\% case is another instance of my patented formula.

The paint drying example, redux:

```{r}
paint %>% 
  group_by(brand) %>% 
  summarize(x_bar = mean(time), samp_var = var(time), n=n()) %>% kable
```

```{r}
paint_fit2 <- paint %>% t.test(time ~ brand, data=.) 
df2 <- paint_fit2$parameter
lower2 <- paint_fit2$conf.int[1]
upper2 <- paint_fit2$conf.int[2]
```

The degrees of freedom is $\nu=`r df2`$. The number from the $t$ distribution is $t_{`r df2`, 0.025} = `r -qt(0.025, df2)`$.

The 95\% confidence interval is $[`r lower2`, `r upper2`]$ (as compared to $[`r lower`, `r upper`]$)

## watching plants grow

Instead of watching paint dry, let's watch plants grow. (Textbook question 9.40.)

20 tree seeds are planted. 10 get a nitrogen fertilizer. After 140 days all the stem growths are measured in grams. The goal is to estimate the mean difference between the two groups.

Here is a summary of the data:
```{r}
nitro <- read.delim("Ex09.40.txt") %>% gather(key=fertilizer, value=weight)
nitro %>% 
  group_by(fertilizer) %>%
  summarize(x_bar = mean(weight), samp_var = var(weight), n=n()) %>% kable
nitro_fit <- nitro %>% 
  t.test(weight ~ fertilizer, data=.)
df3 <- nitro_fit$parameter
lower3 <- nitro_fit$conf.int[1]
upper3 <- nitro_fit$conf.int[2]
```

The degrees of freedom is $\nu=`r df3`$. The number from the $t$ distribution is $t_{`r df3`, 0.025} = `r -qt(0.025, df3)`$.

The 95\% confidence interval is $[`r lower3`, `r upper3`]$.